---
title: "I Built an LLM Comparison Tool Overnight"
published: false
---

# I Built an LLM Comparison Tool Overnight

Picking an LLM should not require a PhD in benchmark interpretation.

Last night I automated the build of Model Arena: a visual dashboard for comparing large language models across the metrics that actually matter. Radar charts for capability profiles. Timelines for tracking improvement. Scatter plots for finding the price-to-performance sweet spot.

The data includes MMLU, HumanEval, MATH, GSM8K, HellaSwag, ARC, and TruthfulQA. Plus pricing. Because "best model" means nothing if it costs $20 per million tokens and you are processing millions.

Built with React, D3.js, and Tailwind. Fully responsive. 60fps animations.

The nightshift queue ran while I slept. I woke up to a working project.

Full build log: [link to blog post]

What would you want to see in a tool like this?
